{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/autofs/homes/005/fd881/repos/MedImaging-ModelDriftMonitoring/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model_drift import mgb_locations\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from model_drift.data import mgb_data\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate windows\n",
    "def generate_windows(start_date, end_date, window_size_days=14, stride_days=1):\n",
    "    current_start = start_date\n",
    "    while current_start + timedelta(days=window_size_days) <= end_date:\n",
    "        current_end = current_start + timedelta(days=window_size_days)\n",
    "        yield current_start, current_end\n",
    "        current_start += timedelta(days=stride_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/autofs/cluster/qtim/projects/xray_drift/analysis/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\n",
    "    mgb_locations.labels_csv,\n",
    ")\n",
    "labels_df['StudyDate'] = pd.to_datetime(labels_df['StudyDate'])\n",
    "labels_df['StudyDate'] = labels_df['StudyDate'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = labels_df.columns[-13:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Counts over Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_counts = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "for i, disease in tqdm(enumerate(diseases), total=len(diseases)):\n",
    "    \n",
    "    disease_df = labels_df[['StudyDate', disease]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "    counts_data = []  # List to store the proportions and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date):\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        positive_cases = window_data[disease].sum()\n",
    "        counts_data.append((window_start, positive_cases))\n",
    "\n",
    "    # Create a DataFrame for the collected proportions and dates\n",
    "    proportions_df = pd.DataFrame(counts_data, columns=['WindowStart', 'Counts'])\n",
    "    results_counts[disease] = proportions_df\n",
    "\n",
    "# Merge all the dataframes into one\n",
    "for disease, df in results_counts.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    df.rename(columns={'Counts': disease}, inplace=True) \n",
    "\n",
    "counts_df = pd.concat(results_counts.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts_df.to_csv(os.path.join(output_dir, 'raw_counts.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(counts_df.columns)\n",
    "cols = 2\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(counts_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(counts_df.index, counts_df[disease])\n",
    "    ax.set_title(disease)\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('Counts in 14 Day Window')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir, 'raw_counts_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevalence over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_prevalence = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "for i, disease in tqdm(enumerate(diseases), total=len(diseases)):\n",
    "    \n",
    "    disease_df = labels_df[['StudyDate', disease]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "    proportions_data = []  # List to store the proportions and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date):\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        positive_cases = window_data[disease].sum()\n",
    "        total_cases = window_data.shape[0]\n",
    "        proportion_positive = positive_cases / total_cases if total_cases > 0 else 0\n",
    "        proportions_data.append((window_start, proportion_positive))\n",
    "\n",
    "    # Create a DataFrame for the collected proportions and dates\n",
    "    proportions_df = pd.DataFrame(proportions_data, columns=['WindowStart', 'Prevalence'])\n",
    "    results_prevalence[disease] = proportions_df\n",
    "\n",
    "# Merge all the dataframes into one\n",
    "for disease, df in results_prevalence.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "prevalence_df = pd.concat(results_prevalence.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prevalence_df.to_csv(os.path.join(output_dir, 'prevalence.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(prevalence_df.columns)\n",
    "cols = 2\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(prevalence_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(prevalence_df.index, prevalence_df[disease])\n",
    "    ax.set_title(disease)\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('Prevalence')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir, 'prevalence_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_prevalence_for_batch(batch_args):\n",
    "    \"\"\"Compute prevalence for a batch of windows.\"\"\"\n",
    "    proportions_data = []\n",
    "    for args in batch_args:\n",
    "        try:\n",
    "            window_start, window_end, disease_df, disease = args\n",
    "            window_data = disease_df.loc[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "            positive_cases = window_data[disease].sum()\n",
    "            total_cases = window_data.shape[0]\n",
    "            proportion_positive = positive_cases / total_cases if total_cases > 0 else 0\n",
    "            proportions_data.append((window_start, proportion_positive))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing window {window_start} - {window_end}: {e}\")\n",
    "    return proportions_data\n",
    "\n",
    "def batch_arguments(arguments, batch_size):\n",
    "    \"\"\"Split a list of arguments into smaller lists of a specified maximum size.\"\"\"\n",
    "    return [arguments[i:i + batch_size] for i in range(0, len(arguments), batch_size)]\n",
    "\n",
    "def calculate_prevalence_for_category(df, category, diseases, batch_size=10, n_jobs=4):\n",
    "    \"\"\"Calculate prevalence for each category and disease combination.\"\"\"\n",
    "    results_prevalence = {}\n",
    "    categories = df[category].dropna().unique()\n",
    "    diseases = ['label.'+ d for d in diseases]\n",
    "    for cat_value in tqdm(categories, desc=f\"Processing {category}\"):\n",
    "    #for cat_value in categories:\n",
    "\n",
    "        if cat_value not in top_5_poc:\n",
    "            print(f\"Skipping {cat_value}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Processing {cat_value}\") \n",
    "\n",
    "        for disease in tqdm(diseases, desc=\"Processing diseases\", leave=False):\n",
    "        \n",
    "        #for disease in diseases:\n",
    "\n",
    "            cat_df = df[df[category] == cat_value].copy()\n",
    "            cat_df['StudyDate'] = pd.to_datetime(cat_df['StudyDate'], errors='coerce')\n",
    "            cat_df.dropna(subset=['StudyDate'], inplace=True)\n",
    "            cat_df.set_index('StudyDate', inplace=True)\n",
    "            \n",
    "            start_date = cat_df.index.min()\n",
    "            end_date = cat_df.index.max()\n",
    "\n",
    "            window_args = [(window_start, window_end, cat_df, disease)\n",
    "                           for window_start, window_end in generate_windows(start_date, end_date, stride_days=1, window_size_days=30)]\n",
    "\n",
    "            batched_args = batch_arguments(window_args, batch_size=batch_size)\n",
    "\n",
    "            with Parallel(n_jobs=n_jobs) as parallel:\n",
    "                all_proportions_data = parallel(delayed(compute_prevalence_for_batch)(batch) for batch in batched_args)\n",
    "\n",
    "            # Flatten the list of lists returned by parallel processing\n",
    "            proportions_data = [item for sublist in all_proportions_data for item in sublist]\n",
    "            proportions_df = pd.DataFrame(proportions_data, columns=['WindowStart', f'{cat_value}_{disease}'])\n",
    "            key = f'{cat_value}_{disease}'\n",
    "            results_prevalence[key] = proportions_df\n",
    "\n",
    "    return results_prevalence\n",
    "\n",
    "def plot_prevalence(performance_df, save_path=None, show=True):\n",
    "    diseases = set(col.split('_')[-1] for col in performance_df.columns)\n",
    "    manufacturers = set(col.split('_')[0] for col in performance_df.columns)\n",
    "\n",
    "    n_diseases = len(diseases)\n",
    "    cols = 2\n",
    "    rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "    plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "    for i, disease in enumerate(sorted(diseases), start=1):\n",
    "        ax = plt.subplot(rows, cols, i)\n",
    "        for manufacturer in sorted(manufacturers):\n",
    "            if manufacturer in ['SIEMENS', 'GE MEDICAL SYSTEMS']:\n",
    "                continue\n",
    "            col_name = f'{manufacturer}_{disease}'\n",
    "            if col_name in performance_df.columns:\n",
    "                avg_performance = performance_df[col_name].mean()\n",
    "                label = f\"{manufacturer} (avg: {avg_performance:.2f})\"\n",
    "                ax.plot(performance_df.index, performance_df[col_name], label=label)\n",
    "        ax.set_title(disease)\n",
    "        ax.set_xlabel('Window Start Date')\n",
    "        ax.set_ylabel('Prevalence in 14 Day Window')\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\n",
    "'Atelectasis',\n",
    "'Consolidation',\n",
    "'Edema',\n",
    "'Lung Lesion',\n",
    "'Lung Opacity',\n",
    "'Pleural Effusion',\n",
    "'Pneumonia',\n",
    "'Pneumothorax'\n",
    "]\n",
    "\n",
    "label_cols = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Lung Opacity', 'Pleural Other', 'Pleural Effusion', 'Pneumonia', 'Pneumothorax', 'Support Devices', ]\n",
    "#label_cols = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Lung Opacity', 'Pleural Other', 'Pleural Effusion', 'Pneumonia', 'Pneumothorax', 'Support Devices', 'Enlarged Cardiomediastinum', ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read jsonl file from pred_folder in df\n",
    "pred_folder = '/autofs/cluster/qtim/projects/xray_drift/inferences/mgb_data_from_chexpert_retrain_frontal_only_lr1e-4_frozen_step25_qwen_labels_allclasses'\n",
    "pred_folder = '/autofs/cluster/qtim/projects/xray_drift/inferences/classification_imagenet_nogamma/'\n",
    "\n",
    "pred_folder = '/autofs/cluster/qtim/projects/xray_drift/inferences/classification_final_allpoc_inference/'\n",
    "\n",
    "df_preds = pd.read_json(os.path.join(pred_folder, 'preds.jsonl'), lines=True)\n",
    "df_preds = pd.concat(\n",
    "    [\n",
    "        df_preds,\n",
    "        pd.DataFrame(df_preds['activation'].values.tolist(), columns=[f\"activation.{c}\" for c in label_cols])\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_preds = pd.concat(\n",
    "    [\n",
    "        df_preds,\n",
    "        pd.DataFrame(df_preds['label'].values.tolist(), columns=[f\"label.{c}\" for c in label_cols])\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dicom = pd.read_csv(mgb_locations.dicom_inventory_csv)\n",
    "\n",
    "def make_index(row: pd.Series):\n",
    "    return f\"{row.PatientID}_{row.AccessionNumber}_{row.SOPInstanceUID}\"\n",
    "\n",
    "# df_dicom only has anonimized dates, so we are pulling them from labels\n",
    "study_dates = labels_df[['StudyInstanceUID','StudyDate']].copy()\n",
    "\n",
    "df_dicom.drop(columns=[\"StudyDate\"], inplace=True)\n",
    "df_dicom = df_dicom.merge(\n",
    "    study_dates,\n",
    "    left_on=\"StudyInstanceUID\",\n",
    "    right_on=\"StudyInstanceUID\",\n",
    ")\n",
    "df_dicom[\"index\"] = df_dicom.apply(make_index, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = df_preds.merge(\n",
    "    df_dicom,\n",
    "    on=\"index\",\n",
    ")\n",
    "\n",
    "#get accesion number from here\n",
    "crosswalk = pd.read_csv(mgb_locations.crosswalk_csv, dtype={\"ANON_AccNumber\": int})\n",
    "crosswalk = crosswalk[[\"ANON_AccNumber\", \"ORIG_AccNumber\"]]\n",
    "\n",
    "# get other metadata from here\n",
    "reports = pd.read_csv(mgb_locations.reports_csv, dtype=str)\n",
    "reports = reports[\n",
    "    [\n",
    "        \"Accession Number\",\n",
    "        \"Point of Care\",\n",
    "        \"Patient Sex\",\n",
    "        \"Patient Age\",\n",
    "        \"Is Stat\",\n",
    "        \"Exam Code\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "df_preds = df_preds.merge(\n",
    "    crosswalk,\n",
    "    how=\"left\",\n",
    "    left_on=\"AccessionNumber\",\n",
    "    right_on=\"ANON_AccNumber\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "df_preds = df_preds.merge(\n",
    "    reports,\n",
    "    how=\"left\",\n",
    "    left_on=\"ORIG_AccNumber\",\n",
    "    right_on=\"Accession Number\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "\n",
    "\n",
    "df_preds['StudyDate'] = pd.to_datetime(df_preds['StudyDate'])\n",
    "df_preds['StudyDate'] = df_preds['StudyDate'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only rows where study date is after 2019/10/21\n",
    "#df_preds = df_preds[df_preds['StudyDate'] > pd.to_datetime('2019-10-21').date()]\n",
    "\n",
    "# exclude laterals \n",
    "df_preds = df_preds[df_preds['ViewPosition'] != 'LL']\n",
    "#df_preds = df_preds[df_preds['StudyDate'] > pd.to_datetime('2019-10-21').date()]\n",
    "df_preds['ViewPosition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_performance = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    # Create a DataFrame for each disease containing only necessary columns\n",
    "    disease_df = df_preds[['StudyDate', disease_label, disease_score]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "    \n",
    "    # Get the overall start and end dates\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, window_size_days=30):\n",
    "    # Select data for the current window\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        score = roc_auc_score(window_data[disease_label], window_data[disease_score])\n",
    "        scores_data.append((window_start, score))\n",
    "\n",
    "    # Create a DataFrame for the collected scores and dates\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{disease}'])\n",
    "    results_performance[disease] = scores_df    \n",
    "\n",
    "    # Merge all the dataframes into one\n",
    "for disease, df in results_performance.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    #df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "performance_df = pd.concat(results_performance.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_scores_for_batch(batch_args):\n",
    "    scores_data = []\n",
    "    for args in batch_args:\n",
    "        window_start, window_end, disease_df, disease_label, disease_score = args\n",
    "        window_data = disease_df.loc[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        try:\n",
    "            score = roc_auc_score(window_data[disease_label], window_data[disease_score])\n",
    "        except ValueError:\n",
    "            score = np.nan\n",
    "        scores_data.append((window_start, score))\n",
    "    return scores_data\n",
    "\n",
    "def batch_arguments(arguments, batch_size):\n",
    "    return [arguments[i:i + batch_size] for i in range(0, len(arguments), batch_size)]\n",
    "\n",
    "def calculate_performance_for_category(df, category, label_cols, batch_size=10, n_jobs=-1):\n",
    "    results_performance = {}\n",
    "    categories = df[category].dropna().unique()\n",
    "\n",
    "\n",
    "\n",
    "    for cat_value in tqdm(categories, desc=f\"Processing {category}\"):\n",
    "\n",
    "        #replace _ in catvalue with nothing\n",
    "        \n",
    "    #for cat_value in categories:\n",
    "\n",
    "        #Hotfix, remove later\n",
    "        if cat_value not in top_5_poc:\n",
    "            print(f\"Skipping {cat_value}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Processing {cat_value}\") \n",
    "\n",
    "        for disease in tqdm(label_cols, desc=\"Processing diseases\", leave=False):\n",
    "        #for disease in label_cols:\n",
    "\n",
    "            disease_label = f'label.{disease}'\n",
    "            disease_score = f'activation.{disease}'\n",
    "            cat_df = df[df[category] == cat_value]\n",
    "            disease_df = cat_df[['StudyDate', disease_label, disease_score]].copy()\n",
    "            disease_df['StudyDate'] = pd.to_datetime(disease_df['StudyDate'], errors='coerce')\n",
    "            disease_df.dropna(subset=['StudyDate'], inplace=True)\n",
    "\n",
    "            disease_df.set_index('StudyDate', inplace=True)\n",
    "            start_date = disease_df.index.min()\n",
    "            end_date = disease_df.index.max()\n",
    "            window_args = [(window_start, window_end, disease_df, disease_label, disease_score)\n",
    "                           for window_start, window_end in generate_windows(start_date, end_date, stride_days=1, window_size_days=30)]\n",
    "\n",
    "            batched_args = batch_arguments(window_args, batch_size=batch_size)\n",
    "\n",
    "            with Parallel(n_jobs=4) as parallel:\n",
    "                all_scores_data = parallel(delayed(compute_scores_for_batch)(batch) for batch in batched_args)\n",
    "\n",
    "            # Flatten the list of lists returned by parallel processing\n",
    "            scores_data = [item for sublist in all_scores_data for item in sublist]\n",
    "            scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{cat_value}_{disease}'])\n",
    "            key = f'{cat_value}_{disease}'\n",
    "            results_performance[key] = scores_df\n",
    "\n",
    "    return results_performance\n",
    "\n",
    "def plot_performance(performance_df, save_path=None, show=True):\n",
    "    diseases = set(col.split('_')[-1] for col in performance_df.columns)\n",
    "    manufacturers = set(col.split('_')[0] for col in performance_df.columns)\n",
    " \n",
    "    manufacturers = ['Varian_4343R', '3543EZE']\n",
    "    n_diseases = len(diseases)\n",
    "    cols = 2\n",
    "    rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "    plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "    for i, disease in enumerate(sorted(diseases), start=1):\n",
    "        ax = plt.subplot(rows, cols, i)\n",
    "        for manufacturer in sorted(manufacturers):\n",
    "            if manufacturer in ['SIEMENS', 'GE MEDICAL SYSTEMS']:\n",
    "                continue\n",
    "            col_name = f'{manufacturer}_{disease}'\n",
    "            if col_name in performance_df.columns:\n",
    "                avg_performance = performance_df[col_name].mean()\n",
    "                label = f\"{manufacturer} (avg: {avg_performance:.2f})\"\n",
    "                ax.plot(performance_df.index, performance_df[col_name], label=label)\n",
    "        ax.set_title(disease)\n",
    "        ax.set_xlabel('Window Start Date')\n",
    "        ax.set_ylabel('AUROC')\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_preds['Point of Care'].value_counts()\n",
    "#\n",
    "## group all the point of Cares that contain OP into one category\n",
    "#df_preds['Point of Care'] = df_preds['Point of Care'].apply(lambda x: 'MGH IMG XR OPX' if 'OP' in x else x)\n",
    "#\n",
    "#df_preds['Point of Care'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the 5 most common point of cares\n",
    "top_5_poc = df_preds['ManufacturerModelName'].value_counts().index[:2].tolist()\n",
    "top_5_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds['ManufacturerModelName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Manufacturer', 'ViewPosition', 'PhotometricInterpretation', 'BitsAllocated']#, 'ManufacturerModelName'] \n",
    "\n",
    "categories = ['ManufacturerModelName']\n",
    "\n",
    "output_dir_groups = os.path.join(output_dir, 'subgroup_analysis_nolaterals')\n",
    "os.makedirs(output_dir_groups, exist_ok=True)  \n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    # Performance in Category\n",
    "    print(f'Processing Performance for : {category}')\n",
    "    results_performance = calculate_performance_for_category(df_preds, category, label_cols)\n",
    "    performance_df = pd.concat([df.set_index('WindowStart') for df in results_performance.values()], axis=1)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = f'{output_dir_groups}/{category}_performance.csv'\n",
    "    performance_df.to_csv(csv_filename)\n",
    "    print(f'Saved performance data to \"{csv_filename}\"')\n",
    "    \n",
    "    # Plot the performance and save the plot to a PNG file\n",
    "    plot_filename = f'{output_dir_groups}/{category}_performance_plot.png'\n",
    "    plot_performance(performance_df, save_path=plot_filename, show=False) \n",
    "    print(f'Saved performance plot to \"{plot_filename}\"')\n",
    "\n",
    "\n",
    "    ##Prevalence in Category\n",
    "    print(f'Processing Prevalence for : {category}')\n",
    "\n",
    "    results_prevalence = calculate_prevalence_for_category(df_preds, category, label_cols)\n",
    "    prevalence_df = pd.concat([df.set_index('WindowStart') for df in results_prevalence.values()], axis=1)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = f'{output_dir_groups}/{category}_prevalence.csv'\n",
    "    prevalence_df.to_csv(csv_filename)\n",
    "    print(f'Saved prevalence data to \"{csv_filename}\"')\n",
    "\n",
    "\n",
    "    # Plot the prevalence and save the plot to a PNG file\n",
    "    plot_filename = f'{output_dir_groups}/{category}_prevalence_plot.png'\n",
    "    plot_prevalence(prevalence_df, save_path=plot_filename, show=False)\n",
    "    print(f'Saved prevalence plot to \"{plot_filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_groups = os.path.join(output_dir, 'subgroup_analysis_nolaterals')\n",
    "\n",
    "#performance_df.to_csv(os.path.join(output_dir_groups, 'performance.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(performance_df.columns)\n",
    "cols = 2\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(performance_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(performance_df.index, performance_df[disease])\n",
    "    ax.set_title(disease)\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir_groups, 'performance_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    # Create a DataFrame for each disease containing only necessary columns\n",
    "    disease_df = df_preds[['StudyDate', disease_label, disease_score]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "    \n",
    "    # Get the overall start and end dates\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date):\n",
    "    # Select data for the current window\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        # select only rows where label is 1 \n",
    "\n",
    "        window_data = window_data[(window_data[disease_label] == 0)]\n",
    "    \n",
    "        score = np.mean(window_data[disease_score])\n",
    "        scores_data.append((window_start, score))\n",
    "\n",
    "    # Create a DataFrame for the collected scores and dates\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{disease}'])\n",
    "    results_scores[disease] = scores_df    \n",
    "\n",
    "    # Merge all the dataframes into one\n",
    "for disease, df in results_scores.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    #df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "scores_df = pd.concat(results_scores.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(scores_df.columns)\n",
    "cols = 2\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(scores_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(scores_df.index, scores_df[disease], label='Negative Label Score')\n",
    "    ax.plot(scores_df_pos.index, scores_df_pos[disease], label='Positive Label Score')\n",
    "\n",
    "    ax.set_title(disease)\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('Mean Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir, 'performance_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Density estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores_raw = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    # Create a DataFrame for each disease containing only necessary columns\n",
    "    disease_df = df_preds[['StudyDate', disease_label, disease_score]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "    \n",
    "    # Get the overall start and end dates\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, stride_days=30):\n",
    "    # Select data for the current window\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        \n",
    "        score = window_data[disease_score].to_numpy()\n",
    "        scores_data.append((window_start, score))\n",
    "\n",
    "    # Create a DataFrame for the collected scores and dates\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{disease}'])\n",
    "    results_scores_raw[disease] = scores_df    \n",
    "\n",
    "    # Merge all the dataframes into one\n",
    "for disease, df in results_scores_raw.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    #df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "scores_raw_df = pd.concat(results_scores_raw.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming scores_raw_df exists and is your DataFrame\n",
    "\n",
    "num_columns = 6\n",
    "num_diseases = len(scores_raw_df.columns)\n",
    "num_rows = int(np.ceil(num_diseases / num_columns))\n",
    "\n",
    "fig = make_subplots(rows=num_rows, cols=num_columns, shared_xaxes=True, shared_yaxes=True,\n",
    "                    subplot_titles=scores_raw_df.columns.to_list())\n",
    "\n",
    "for i, disease in enumerate(scores_raw_df.columns, start=1):\n",
    "    row_position = int(np.ceil(i / num_columns))\n",
    "    col_position = i - (row_position - 1) * num_columns\n",
    "    # Extract scores and dates for this disease\n",
    "    scores_list = []\n",
    "    dates_list = []\n",
    "\n",
    "    for window_start, row in scores_raw_df.iterrows():\n",
    "        scores = row[disease]\n",
    "        for score in scores:\n",
    "            scores_list.append(score)\n",
    "            dates_list.append(window_start)\n",
    "\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Score': scores_list,\n",
    "        'Date': pd.to_datetime(dates_list)\n",
    "    })\n",
    "\n",
    "    # Group data by Date and compute KDE for each group\n",
    "    for date, group in combined_df.groupby('Date'):\n",
    "        # KDE computation\n",
    "        if len(group['Score']) > 1:  # KDE requires at least 2 data points\n",
    "            kde = gaussian_kde(group['Score'])\n",
    "            score_range = np.linspace(min(group['Score']), max(group['Score']), 100)\n",
    "            density = kde(score_range)\n",
    "            # Plot KDE as a line\n",
    "            fig.add_trace(go.Scatter(x=score_range, y=density, mode='lines', name=date.strftime('%Y-%m-%d'), legendgroup=str(date), showlegend=(i==1)), row=row_position, col=col_position)\n",
    "\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(height=400*num_rows, width=400*num_columns, title_text=\"KDE of Disease Scores Over Time\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Determine the number of subplots needed based on the number of diseases\n",
    "num_diseases = len(scores_raw_df.columns)\n",
    "num_cols = 3 \n",
    "num_rows = int(np.ceil(num_diseases / num_cols))\n",
    "\n",
    "\n",
    "# Create a large figure to hold all subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 5*num_rows))\n",
    "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Iterate over the columns (diseases) and axes simultaneously\n",
    "for ax, disease in zip(axes, scores_raw_df.columns):\n",
    "    \n",
    "    scores_list = []\n",
    "    dates_list = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for window_start, row in scores_raw_df.iterrows():\n",
    "        # Extract the scores for the current disease\n",
    "        scores = row[disease]  # Get the list of scores for the current disease\n",
    "        \n",
    "        # Append each score to the scores list and the corresponding date to the dates list\n",
    "        for score in scores:\n",
    "            scores_list.append(score)\n",
    "            dates_list.append(window_start)\n",
    "\n",
    "    # Create a new DataFrame with scores and dates for the current disease\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Score': scores_list,\n",
    "        'Date': pd.to_datetime(dates_list)\n",
    "    })\n",
    "\n",
    "    # Group by the Date to plot each distribution for the current disease on its axis\n",
    "    for date, group in combined_df.groupby('Date'):\n",
    "        sns.kdeplot(group['Score'], ax=ax, label=date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    # Set the title for the current subplot\n",
    "    ax.set_title(f'{disease}')\n",
    "\n",
    "    # Hide x-axis labels and legend for individual plots for clarity\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    if ax.legend_:\n",
    "        ax.legend_.remove()\n",
    "\n",
    "# Outside of the loop, set a common X and Y label\n",
    "fig.text(0.5, 0.04, 'Score', ha='center', va='center')\n",
    "fig.text(0.04, 0.5, 'Density', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "# Add a single legend at the bottom of the figure\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=3)\n",
    "\n",
    "# Adjust layout to prevent overlap and to allocate space for the legend\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Show the combined figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precovid = df_preds[df_preds['StudyDate'] < pd.to_datetime('2020-04-01').date()].copy()\n",
    "df_postcovid = df_preds[df_preds['StudyDate'] > pd.to_datetime('2020-04-01').date()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in key_columns:\n",
    "    print(i)\n",
    "    print(df_postcovid[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_columns = ['PhotometricInterpretation',\n",
    "               'BitsStored', 'ViewPosition']\n",
    "\n",
    "# Create a composite key in both DataFrames\n",
    "df_precovid['composite_key'] = df_precovid[key_columns].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "df_postcovid['composite_key'] = df_postcovid[key_columns].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "# Since StratifiedShuffleSplit expects arrays, we convert our DataFrame columns to arrays.\n",
    "# Using indices as features just to comply with the expected input shape\n",
    "X = df_precovid.index.to_numpy().reshape(-1, 1)\n",
    "y = df_precovid['composite_key'].to_numpy()  # The composite key acts as the label for stratification\n",
    "\n",
    "# Generate indices for a stratified sample\n",
    "for train_idx, test_idx in sss.split(X, y):\n",
    "    # Correctly indexing df_precovid to get the stratified sample\n",
    "    df_precovid_sampled = df_precovid.iloc[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_precovid_sampled, df_postcovid], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_performance = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    # Create a DataFrame for each disease containing only necessary columns\n",
    "    disease_df = df_combined[['StudyDate', disease_label, disease_score]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "    \n",
    "    # Get the overall start and end dates\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date):\n",
    "    # Select data for the current window\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        score = roc_auc_score(window_data[disease_label], window_data[disease_score])\n",
    "        scores_data.append((window_start, score))\n",
    "\n",
    "    # Create a DataFrame for the collected scores and dates\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{disease}'])\n",
    "    results_performance[disease] = scores_df    \n",
    "\n",
    "    # Merge all the dataframes into one\n",
    "for disease, df in results_performance.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    #df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "performance_df_sampled = pd.concat(results_performance.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(performance_df_sampled.columns)\n",
    "cols = 2\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(performance_df_sampled.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(performance_df_sampled.index, performance_df_sampled[disease])\n",
    "    ax.set_title(disease)\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir, 'performance_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cramér's V function\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Select only categorical columns\n",
    "categorical_columns = ['Manufacturer', 'ViewPosition', 'PhotometricInterpretation', 'BitsAllocated', 'Point of Care', 'Is Stat']\n",
    "\n",
    "# Initialize an empty DataFrame to store Cramér's V values\n",
    "cramers_v_matrix = pd.DataFrame(index=categorical_columns, columns=categorical_columns, dtype=float)\n",
    "\n",
    "# Calculate Cramér's V for each pair of categorical columns and fill the matrix\n",
    "for col1 in categorical_columns:\n",
    "    for col2 in categorical_columns:\n",
    "        cramers_v_matrix.loc[col1, col2] = cramers_v(df_preds[col1], df_preds[col2])\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_v_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Cramér's V Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut-Off Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select studies after train date end but before val date end\n",
    "df_ref = df_preds[(df_preds['StudyDate'] > mgb_data.TRAIN_DATE_END.date())&(df_preds['StudyDate'] < mgb_data.VAL_DATE_END.date())]\n",
    "\n",
    "# calcuate the ideal cutoff point for each disease to maximize f1-score on this data\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "cut_offs = {}  \n",
    "\n",
    "for disease in label_cols:\n",
    "    best_f1_score = -1 \n",
    "    best_cutoff = 0  \n",
    "    \n",
    "    for cutoff in np.arange(0, 1, 0.01):\n",
    "        preds = df_ref[f'activation.{disease}'] > cutoff\n",
    "        current_f1_score = f1_score(df_ref[f'label.{disease}'], preds)\n",
    "        \n",
    "        if current_f1_score > best_f1_score:\n",
    "            best_f1_score = current_f1_score\n",
    "            best_cutoff = cutoff\n",
    "    \n",
    "    cut_offs[disease] = {'Max F1 Score': best_f1_score, 'Optimal Cutoff': best_cutoff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "operating_points = {}\n",
    "for disease in label_cols:\n",
    "    fpr, tpr, thresholds = roc_curve(df_ref[f'label.{disease}'], df_ref[f'activation.{disease}'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Calculate Youden Index\n",
    "    youden_index = tpr - fpr\n",
    "    best_index = np.argmax(youden_index)\n",
    "    best_cutoff = thresholds[best_index]\n",
    "\n",
    "    operating_points[disease] = best_cutoff\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f'{disease} AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    #plt.plot(fpr[best_index], tpr[best_index], 'v', markersize=8, fillstyle='none', c='r', label=f'Youden {disease}')\n",
    "\n",
    "# figure size 10 10 \n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate Accuracy, F1-score, Sensitivity, and Specificity for each disease\n",
    "for disease in label_cols:\n",
    "    cutoff = operating_points[disease]\n",
    "    preds = df_ref[f'activation.{disease}'] > cutoff\n",
    "    accuracy = accuracy_score(df_ref[f'label.{disease}'], preds)\n",
    "    f1 = f1_score(df_ref[f'label.{disease}'], preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(df_ref[f'label.{disease}'], preds).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(f'Disease: {disease}, Cut-Off: {cutoff}, Accuracy: {accuracy:.3f}, F1-score: {f1:.3f}, Sensitivity: {sensitivity:.3f}, Specificity: {specificity:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_performance = {}  # Dictionary to hold dataframes for each disease\n",
    "micro_labels = []  # List to collect all labels for micro averaging\n",
    "micro_preds = []  # List to collect all binarized predictions for micro averaging\n",
    "micro_scores = []  # List to collect all scores for AUROC calculation\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    disease_df = df_preds[['StudyDate', disease_label, disease_score]].copy()\n",
    "    disease_df.set_index('StudyDate', inplace=True)\n",
    "\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    optimal_cutoff = operating_points[disease]  # Get the optimal cutoff for the disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, window_size_days=30):\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        binarized_preds = (window_data[disease_score] > optimal_cutoff).astype(int)\n",
    "\n",
    "        # Append to lists for micro averaging\n",
    "        micro_labels.extend(window_data[disease_label].tolist())\n",
    "        micro_preds.extend(binarized_preds.tolist())\n",
    "        micro_scores.extend(window_data[disease_score].tolist())  # Raw scores for AUROC\n",
    "\n",
    "        accuracy = accuracy_score(window_data[disease_label], binarized_preds)\n",
    "        f1 = f1_score(window_data[disease_label], binarized_preds, zero_division=1)\n",
    "        auroc = roc_auc_score(window_data[disease_label], window_data[disease_score]) if window_data[disease_label].nunique() > 1 else float('nan')\n",
    "\n",
    "        scores_data.append((window_start, accuracy, f1, auroc))\n",
    "\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{disease}_Accuracy', f'{disease}_F1', f'{disease}_AUROC'])\n",
    "    results_performance[disease] = scores_df\n",
    "\n",
    "\n",
    "\n",
    "performance_df['Macro_Accuracy'] = performance_df.filter(like='_Accuracy').mean(axis=1)\n",
    "performance_df['Macro_F1'] = performance_df.filter(like='_F1').mean(axis=1)\n",
    "performance_df['Macro_AUROC'] = performance_df.filter(like='_AUROC').mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "date_format = mdates.DateFormatter('%Y-%m')\n",
    "month_locator = mdates.MonthLocator(interval=3)\n",
    "# Dont include training data in plot\n",
    "filtered_df = performance_df#[performance_df.index >= pd.Timestamp('2019-10-01')]\n",
    "\n",
    "n_diseases = len(label_cols)  # Assuming label_cols list exists with the names of diseases\n",
    "cols = 2\n",
    "rows = int(np.ceil((n_diseases + 2) / cols))  # Adding two more plots for micro and macro averages\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "# Plot metrics for each disease from October 2019 onwards\n",
    "for i, disease in enumerate(label_cols, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(filtered_df.index, filtered_df[f'{disease}_Accuracy'], label='Accuracy', color='blue')\n",
    "    ax.plot(filtered_df.index, filtered_df[f'{disease}_F1'], label='F1 Score', color='green')\n",
    "    ax.plot(filtered_df.index, filtered_df[f'{disease}_AUROC'], label='AUROC', color='red')\n",
    "\n",
    "    ax.set_title(f'Metrics for {disease}')\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('Metric Values')\n",
    "    ax.set_xlim(pd.to_datetime('2019-10-01').date(), pd.to_datetime('2021-07-01').date())\n",
    "    ax.xaxis.set_major_locator(month_locator)\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "# Macro average plot\n",
    "ax = plt.subplot(rows, cols, n_diseases + 2)\n",
    "ax.plot(filtered_df.index, filtered_df['Macro_Accuracy'], label='Macro Accuracy', color='blue')\n",
    "ax.plot(filtered_df.index, filtered_df['Macro_F1'], label='Macro F1', color='green')\n",
    "ax.plot(filtered_df.index, filtered_df['Macro_AUROC'], label='Macro AUROC', color='red')\n",
    "ax.set_xlim(pd.to_datetime('2019-10-01').date(), pd.to_datetime('2021-07-01').date())\n",
    "ax.xaxis.set_major_locator(month_locator)\n",
    "ax.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "ax.set_title('Macro Average Metrics')\n",
    "ax.set_xlabel('Window Start Date')\n",
    "ax.set_ylabel('Metric Values')\n",
    "plt.xticks(rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# Uncomment the next line to save the figure if needed\n",
    "# plt.savefig(os.path.join(output_dir_groups, 'performance_findings.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model calibration\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# Calibrate your model\n",
    "X_ref = df_ref[['activation.Support Devices']].values  # Make sure 'activation' is in the correct shape\n",
    "y_ref = df_ref['label.Support Devices'].values\n",
    "\n",
    "X_logistic = X_ref.reshape(-1, 1)  # Ensure it's 2D\n",
    "y_logistic = y_ref\n",
    "\n",
    "# Fit logistic regression for calibration\n",
    "log_reg = LogisticRegression().fit(X_logistic, y_logistic)\n",
    "\n",
    "# Apply the calibration\n",
    "calibrated_probs_log = log_reg.predict_proba(df_preds['activation.Support Devices'].values.reshape(-1, 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[['activation.Support Devices', 'calibrated_probs.Support Devices', 'label.Support Devices', ]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds['calibrated_probs.Support Devices'] = calibrated_probs_log\n",
    "\n",
    "# calculate performance\n",
    "results_performance = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "\n",
    "binarized_preds = (df_preds['activation.Support Devices'] > 0.3).astype(int)\n",
    "binarized_preds_cal = (df_preds['calibrated_probs.Support Devices'] > 0.3).astype(int)\n",
    "\n",
    "score = roc_auc_score(df_preds['label.Support Devices'], df_preds['activation.Support Devices'])\n",
    "score_cal = roc_auc_score(df_preds['label.Support Devices'], df_preds['calibrated_probs.Support Devices'])\n",
    "\n",
    "print(f'Accuracy before calibration: {score:.2f}')\n",
    "print(f'Accuracy after calibration: {score_cal:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_preds is your DataFrame, and it includes 'StudyDate', 'label.DiseaseA', and 'activation.DiseaseA'\n",
    "disease = 'Support Devices'\n",
    "disease_label = f'label.{disease}'\n",
    "disease_score = f'calibrated_probs.{disease}'\n",
    "\n",
    "# Assuming cut_offs is your dictionary with optimal cutoffs\n",
    "optimal_cutoff = cut_offs[disease]['Optimal Cutoff']\n",
    "\n",
    "# Filter data for the specific disease\n",
    "disease_df = df_preds[['StudyDate', disease_label, disease_score]].copy()\n",
    "disease_df['StudyDate'] = pd.to_datetime(disease_df['StudyDate'])\n",
    "disease_df.set_index('StudyDate', inplace=True)\n",
    "\n",
    "# Sort the DataFrame by date to ensure chronological order\n",
    "disease_df.sort_index(inplace=True)\n",
    "\n",
    "# Define the start and end dates if not done already\n",
    "start_date = disease_df.index.min()\n",
    "end_date = disease_df.index.max()\n",
    "\n",
    "# Function to generate windows (assuming you have this function)\n",
    "# If not, here's a simple implementation:\n",
    "\n",
    "# Calculate accuracy in each window\n",
    "scores_data = []\n",
    "for window_start, window_end in generate_windows(start_date, end_date, window_size_days=14):\n",
    "    window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "    binarized_preds = (window_data[disease_score] > optimal_cutoff).astype(int)\n",
    "    \n",
    "    score = accuracy_score(window_data[disease_label], binarized_preds)\n",
    "    scores_data.append((window_start, score))\n",
    "\n",
    "# Create a DataFrame for the collected scores and dates\n",
    "scores_df = pd.DataFrame(scores_data, columns=['WindowStart', 'Accuracy'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scores_df['WindowStart'], scores_df['Accuracy'], linestyle='-')\n",
    "plt.title(f'Accuracy Over Time for {disease}')\n",
    "plt.xlabel('Window Start Date')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Digging into Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are looking at the largest site: MGH IMG XR ER MG WHT1, about 60000 images (2/3) of our dataset come from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new label for point of care that combines all the OPs\n",
    "df_preds['Point of Care_combined'] = df_preds['Point of Care'].apply(lambda x: 'MGH IMG XR OPX' if 'OP' in x else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one specific site \n",
    "\n",
    "site = 'MGH IMG XR ER MG WHT1'\n",
    "site_disp = site.replace(' ', '~')\n",
    "\n",
    "print(df_preds['Point of Care'].value_counts())\n",
    "\n",
    "df_er = df_preds[df_preds['Point of Care'] == site].copy()\n",
    "\n",
    "df_er['StudyDate'] = pd.to_datetime(df_er['StudyDate'])\n",
    "df_er.set_index('StudyDate', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_counts = {}\n",
    "\n",
    "# Get the overall start and end dates\n",
    "start_date = df_er.index.min()\n",
    "end_date = df_er.index.max()\n",
    "\n",
    "window_size_days = 30 \n",
    "# Loop through each time window and count exams\n",
    "for window_start, window_end in generate_windows(start_date, end_date, window_size_days=window_size_days):\n",
    "    window_data = df_er[(df_er.index >= window_start) & (df_er.index < window_end)]\n",
    "    count = window_data.shape[0]/window_size_days  # Count the number of rows (exams) in the window, normalized by window size\n",
    "    exam_counts[window_start] = {'ExamCount': count}\n",
    "\n",
    "# Convert the exam counts dictionary to a DataFrame\n",
    "exam_counts_df = pd.DataFrame(exam_counts).T\n",
    "\n",
    "# If needed, fill missing values with 0\n",
    "exam_counts_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "\n",
    "plt.plot(exam_counts_df.index, exam_counts_df['ExamCount'])\n",
    "\n",
    "plt.title(f'Exams per day at $\\mathbf{{{site_disp}}}$')\n",
    "plt.xlabel('Window Start Date')\n",
    "plt.ylabel('Count per Day')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.tight_layout()  \n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_er.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=45)\n",
    "plt.hist(df_er['Patient Age'], bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'ViewPosition'\n",
    "\n",
    "\n",
    "# check the machines used\n",
    "print(df_er[parameter].value_counts())\n",
    "\n",
    "# plot the proportion of the machines used over time for that location\n",
    "\n",
    "\n",
    "# Get the overall start and end dates\n",
    "start_date = df_er.index.min()\n",
    "end_date = df_er.index.max()\n",
    "\n",
    "proportion_data = {}\n",
    "\n",
    "for window_start, window_end in generate_windows(start_date, end_date, window_size_days=30):\n",
    "    window_data = df_er[(df_er.index >= window_start) & (df_er.index < window_end)]\n",
    "    proportions = window_data[parameter].value_counts(normalize=False)\n",
    "    proportion_data[window_start] = proportions/30\n",
    "    \n",
    "machine_df = pd.DataFrame(proportion_data).T\n",
    "\n",
    "machine_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Fluorospot, which has only two non nans rows\n",
    "\n",
    "#count non nans in each column\n",
    "print(machine_df.count())\n",
    "\n",
    "#machine_df.drop(columns=['Fluorospot Compact FD'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "\n",
    "for machine in machine_df.columns:\n",
    "    plt.plot(machine_df.index, machine_df[machine], label=machine)\n",
    "\n",
    "plt.title(f'{parameter} over Time at $\\mathbf{{{site_disp}}}$')\n",
    "plt.xlabel('Window Start Date')\n",
    "plt.ylabel('Count per Day')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.legend()\n",
    "plt.tight_layout()  \n",
    "\n",
    "# plt.savefig(os.path.join(output_dir, 'performance_findings.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "palettes = ['deep', 'bright', 'pastel', 'muted', 'dark', 'colorblind']\n",
    "colors = []\n",
    "\n",
    "machine_df_prop = machine_df.div(machine_df.sum(axis=1), axis=0).copy()\n",
    "for pal in palettes:\n",
    "    # Extend the color list with colors from each palette\n",
    "    colors.extend(sns.color_palette(pal))\n",
    "\n",
    "plt.stackplot(machine_df_prop.index, *machine_df_prop.T.values, labels=machine_df_prop.columns, colors=colors)\n",
    "plt.xlim(machine_df_prop.index.min(), machine_df_prop.index.max())\n",
    "plt.ylim(0, 1)  \n",
    "\n",
    "plt.title(f'{parameter} over Time at $\\mathbf{{{site_disp}}}$')\n",
    "plt.xlabel('Window Start Date')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()  \n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(os.path.join(output_dir, 'performance_findings_stacked.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select studies that were done on the Varian_4343R machine and look at the view positions\n",
    "\n",
    "for machine in machine_df.columns:\n",
    "    print(f'\\nMachine: {machine}')\n",
    "    print(df_er[df_er['ManufacturerModelName'] == machine].ViewPosition.value_counts())\n",
    "\n",
    "#for machine in machine_df.columns:\n",
    "#    print(f'\\nMachine: {machine}')\n",
    "#    print(df_er[df_er['ManufacturerModelName'] == machine].PhotometricInterpretation.value_counts())\n",
    "\n",
    "#for machine in machine_df.columns:\n",
    "#    print(f'\\nMachine: {machine}')\n",
    "#    print(df_er[df_er['ManufacturerModelName'] == machine].XRayTubeCurrent.median())\n",
    "\n",
    "for machine in machine_df.columns:\n",
    "    print(f'\\nMachine: {machine}')\n",
    "    print(df_er[df_er['ManufacturerModelName'] == machine]['PhotometricInterpretation'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'PhotometricInterpretation'\n",
    "\n",
    "machine_bits = {}\n",
    "for machine in machine_df.columns:\n",
    "\n",
    "    print(f'\\nMachine: {machine}')\n",
    "    machine_df_sel = df_er[df_er['ManufacturerModelName'] == machine]\n",
    "\n",
    "    proportion_data_view = {}\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, window_size_days=30):\n",
    "        window_data = machine_df_sel[(machine_df_sel.index >= window_start) & (machine_df_sel.index < window_end)]\n",
    "        \n",
    "        proportions = window_data[parameter].value_counts()\n",
    "        \n",
    "        proportion_data_view[window_start] = proportions/30\n",
    "        \n",
    "    \n",
    "    machine_bits[machine] = pd.DataFrame(proportion_data_view).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over items in the dict and plot each df\n",
    "for machine, df in machine_bits.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    #palette = sns.color_palette(\"flare\", n_colors=len(df.columns))\n",
    "    #colors = sns.color_palette(palette, n_colors=len(df.columns))\n",
    "   \n",
    "    palettes = ['deep', 'bright', 'pastel', 'muted', 'dark', 'colorblind']\n",
    "    colors = []\n",
    "\n",
    "    for pal in palettes:\n",
    "        # Extend the color list with colors from each palette\n",
    "        colors.extend(sns.color_palette(pal))\n",
    "\n",
    "    df_prop = df.div(df.sum(axis=1), axis=0).copy()\n",
    "\n",
    "    plt.stackplot(df_prop.index, *df_prop.T.values, labels=df_prop.columns, colors=colors)\n",
    "    plt.xlim(df.index.min(), df.index.max())\n",
    "    plt.ylim(0, 1)  \n",
    "    machine = machine.replace('_', '\\_')\n",
    "    plt.title(f'{parameter} over Time for $\\mathbf{{{machine}}}$ at $\\mathbf{{{site_disp}}}$')\n",
    "    plt.xlabel('Window Start Date')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='lower left')  \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.savefig(os.path.join(output_dir, 'performance_findings_stacked.png'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over items in the dict and plot each df\n",
    "for machine, df in machine_bits.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    #palette = sns.color_palette(\"flare\", n_colors=len(df.columns))\n",
    "    #colors = sns.color_palette(palette, n_colors=len(df.columns))\n",
    "   \n",
    "    palettes = ['deep', 'bright', 'pastel', 'muted', 'dark', 'colorblind']\n",
    "    colors = []\n",
    "\n",
    "    for pal in palettes:\n",
    "        # Extend the color list with colors from each palette\n",
    "        colors.extend(sns.color_palette(pal))\n",
    "    # select only top 5\n",
    "    cols_use = df.sum().nlargest(5).keys().tolist()\n",
    "    df = df[cols_use]\n",
    "    plt.plot(df.index, df, )\n",
    "    #plt.xlim(df.index.min(), df.index.max())\n",
    "    #plt.ylim(0, 1)  \n",
    "    machine = machine.replace('_', '\\_')\n",
    "    plt.title(f'{parameter} over Time for $\\mathbf{{{machine}}}$ at $\\mathbf{{{site_disp}}}$')\n",
    "    plt.xlabel('Window Start Date')\n",
    "    plt.ylabel('Count per day')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(cols_use)  \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.savefig(os.path.join(output_dir, 'performance_findings_stacked.png'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance for that machine\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_er_machine = df_er[df_er['PhotometricInterpretation'] == 'MONOCHROME2'].copy()\n",
    "\n",
    "results_performance = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    # Create a DataFrame for each disease containing only necessary columns\n",
    "    disease_df = df_er[[ disease_label, disease_score]].copy()\n",
    "    #disease_df.set_index('StudyDate', inplace=True) \n",
    "    \n",
    "    # Get the overall start and end dates\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, window_size_days=60):\n",
    "    # Select data for the current window\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "\n",
    "        try:\n",
    "            score = roc_auc_score(window_data[disease_label], window_data[disease_score])\n",
    "        except ValueError:\n",
    "            score = np.nan\n",
    "        scores_data.append((window_start, score))\n",
    "\n",
    "    # Create a DataFrame for the collected scores and dates\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{disease}'])\n",
    "    results_performance[disease] = scores_df    \n",
    "\n",
    "    # Merge all the dataframes into one\n",
    "for disease, df in results_performance.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    #df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "performance_df = pd.concat(results_performance.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(performance_df.columns)\n",
    "cols = 6\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(performance_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(performance_df.index, performance_df[disease])\n",
    "    ax.set_title(f'{disease} at $\\mathbf{{{site_disp}}}$')\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir_groups, 'performance_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_er_machine = df_er[df_er['ManufacturerModelName'] == 'Pixium_4343E_CSI'].copy()\n",
    "\n",
    "\n",
    "results_prevalence = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "\n",
    "\n",
    "for i, disease in tqdm(enumerate(label_cols), total=len(label_cols)):\n",
    "    disease_label = f'label.{disease}'\n",
    "    disease_score = f'activation.{disease}'\n",
    "    disease_df = df_er[disease_label].copy()\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "    proportions_data = []  # List to store the proportions and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, window_size_days=30):\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "        positive_cases = window_data.sum()\n",
    "        total_cases = window_data.shape[0]\n",
    "        proportion_positive = positive_cases / total_cases if total_cases > 0 else 0\n",
    "        proportions_data.append((window_start, proportion_positive))\n",
    "\n",
    "    # Create a DataFrame for the collected proportions and dates\n",
    "    proportions_df = pd.DataFrame(proportions_data, columns=['WindowStart', 'Prevalence'])\n",
    "    results_prevalence[disease] = proportions_df\n",
    "\n",
    "# Merge all the dataframes into one\n",
    "for disease, df in results_prevalence.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "prevalence_df = pd.concat(results_prevalence.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(prevalence_df.columns)\n",
    "cols = 6\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(prevalence_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(prevalence_df.index, prevalence_df[disease])\n",
    "    ax.set_title(f'{disease} at $\\mathbf{{{site_disp}}}$')\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel(f'Prevalence in Window')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir_groups, 'performance_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_er['ViewPosition'].dropna().unique().tolist()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_er['ManufacturerModelName'].dropna().unique().tolist()\n",
    "\n",
    "results_performance = {}  # Dictionary to hold dataframes for each disease\n",
    "\n",
    "\n",
    "for cat_value in categories:\n",
    "    disease_label = f'label.Support Devices'\n",
    "    disease_score = f'activation.Support Devices'\n",
    "    # Create a DataFrame for each disease containing only necessary columns\n",
    "    disease_df = df_er[df_er['ManufacturerModelName'] == cat_value][[ disease_label, disease_score]].copy()\n",
    "    #disease_df = df_er[[ disease_label, disease_score]].copy()\n",
    "    #disease_df.set_index('StudyDate', inplace=True) \n",
    "    \n",
    "    # Get the overall start and end dates\n",
    "    start_date = disease_df.index.min()\n",
    "    end_date = disease_df.index.max()\n",
    "\n",
    "    scores_data = []  # List to store the scores and window starts for this disease\n",
    "\n",
    "    for window_start, window_end in generate_windows(start_date, end_date, window_size_days=60):\n",
    "    # Select data for the current window\n",
    "        window_data = disease_df[(disease_df.index >= window_start) & (disease_df.index < window_end)]\n",
    "\n",
    "        try:\n",
    "            score = roc_auc_score(window_data[disease_label], window_data[disease_score])\n",
    "        except ValueError:\n",
    "            score = np.nan\n",
    "        scores_data.append((window_start, score))\n",
    "\n",
    "    # Create a DataFrame for the collected scores and dates\n",
    "    scores_df = pd.DataFrame(scores_data, columns=['WindowStart', f'{cat_value}'])\n",
    "    results_performance[cat_value] = scores_df    \n",
    "\n",
    "    # Merge all the dataframes into one\n",
    "for disease, df in results_performance.items():\n",
    "    df.set_index('WindowStart', inplace=True)  \n",
    "    #df.rename(columns={'Prevalence': disease}, inplace=True) \n",
    "\n",
    "performance_df = pd.concat(results_performance.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diseases = len(performance_df.columns)\n",
    "cols = 6\n",
    "rows = int(np.ceil(n_diseases / cols))\n",
    "\n",
    "plt.figure(figsize=(cols * 6, rows * 4))\n",
    "\n",
    "for i, disease in enumerate(performance_df.columns, start=1):\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.plot(performance_df.index, performance_df[disease])\n",
    "    ax.set_title(f'{disease} at $\\mathbf{{{site_disp}}}$')\n",
    "    ax.set_xlabel('Window Start Date')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    ax.set_xlim(pd.to_datetime('2019-10-01').date(), pd.to_datetime('2021-07-01').date())\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.xaxis.set_major_locator(month_locator)\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir_groups, 'performance_findings.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "date_format = mdates.DateFormatter('%Y-%m')\n",
    "month_locator = mdates.MonthLocator(interval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "\n",
    "for machine in performance_df.columns:\n",
    "    plt.plot(performance_df.index, performance_df[machine], label=machine)\n",
    "\n",
    "plt.title(f'Performance over Time at $\\mathbf{{{site_disp}}}$')\n",
    "plt.xlabel('Window Start Date')\n",
    "plt.ylabel('Count per Day')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.xlim(pd.to_datetime('2019-10-01').date(), pd.to_datetime('2021-07-01').date())\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()  \n",
    "\n",
    "# plt.savefig(os.path.join(output_dir, 'performance_findings.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-image-drift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
